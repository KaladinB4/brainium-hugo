---
title: "When AI Writes Your Words, Who Is Speaking?"
description: "Erik Johannes Husom's critique of 'outsourcing thinking' reveals three categories where AI assistance erodes the value of human effort—and why the solution isn't better tools, but clearer boundaries."
date: 2026-02-01
draft: true
tags: ["essays", "ai-ethics", "communication", "authenticity"]
related_research:
  - "Externalized cognition theory (Clark & Chalmers, Wegner)"
  - "Google Effect studies (Sparrow et al., 2011)"
  - "GPS navigation and hippocampal atrophy (Bohbot et al., 2020)"
---

## The Words Are the Meaning

In January 2026, Erik Johannes Husom published a careful critique of what he calls "outsourcing thinking"—the practice of letting large language models handle cognitive tasks we once did ourselves. He wasn't writing about AI safety or economic displacement. He was writing about something more personal: when you let a machine write your words, something fundamental changes in how you relate to others, and to yourself.

Husom's argument rests on a simple premise: "The words *are* the meaning. Changing the phrasing changes the message." This isn't poetic license. Research on externalized cognition shows we process information differently when we believe it originated externally versus internally. The Google Effect (Sparrow et al., 2011) demonstrated that when people expect to access information later via search, they don't encode it deeply—they encode where to find it. The information never becomes truly theirs.

The parallel to AI writing assistance is uncomfortable but clear. When you prompt a model to "make this sound better" or "help me express this," you're not just editing—you're inviting an external processor to stand between your intent and your expression. And as Husom notes, there's an "exceptionally thin line between getting help with spelling or grammar, and having the model essentially write *for* you."

## Three Categories Where AI Erodes Value

Husom identifies three broad domains where outsourcing thinking carries particular risks. His taxonomy is useful not because it prescribes rules, but because it clarifies trade-offs.

### Personal Communication

The most immediate concern is direct human communication. When we write to someone—an email, a message, a dating app response—there's an implicit contract: these words come from me. They reflect who I am, how I think, what I value. Letting an LLM transform those words breaks that contract.

Husom argues this extends beyond intimate contexts. Any text with a personal sender addressing a human audience carries role expectations and trust. The Norwegian media debate he references—about undisclosed AI use in public writing—speaks to a growing recognition that readers deserve to know when they're receiving machine-transformed content. The words may be "better" by some metric (clearer, more grammatical, more polished), but they no longer carry the weight of human intent.

The deeper issue is developmental. "We rob ourselves of the opportunity to grow and learn, without training wheels." Writing isn't just output; it's thinking. The struggle to find the right phrase, the false starts, the awkward constructions you eventually smooth out—these are the process by which you develop your voice. Shortcut that process, and you may never discover "who we can be and become when we stand on our own two feet."

### Valuable Experiences

The second category is more subtle. Husom resists the modern tendency to treat everything as a chore to be optimized. When LLM providers advertise using chatbots to plan vacations, organize parties, or draft personal messages, he sees a fundamental misunderstanding of what makes us human.

"Humans are surprisingly good at finding discontentment in nearly anything." The modern condition includes an expectation that we should be able to do anything we want, anytime—and more importantly, avoid anything we don't feel like doing. But this attitude leads to treating life itself as a series of tasks to be automated away.

Planning a vacation isn't just logistics. It's anticipation, daydreaming, weighing trade-offs, imagining yourself in possible futures. Organizing a party isn't just coordination. It's care, attention to relationships, the satisfaction of bringing people together. These activities are valuable not despite their effort, but because of it. Automate them, and you don't gain time for "more meaningful" activities—you lose the meaning that was already there.

### Tacit Knowledge Building

Husom's third category—building complex tacit knowledge—draws on the strongest evidence. He uses jazz improvisation as his example, learned from a piano tutor: "Good improvisation comes not from just practicing improvisation. You need to play existing songs and tunes, many of them, over and over, learn them by heart, get the chord progressions and motifs under your skin."

This maps directly to established cognitive science. Expertise requires the development of intuitive pattern recognition that can only come from extensive, often repetitive, practice. The knowledge gets encoded not just as explicit facts, but as embodied intuition—the kind that lets a chess master instantly recognize a good position, or a programmer smell a bug before they've consciously analyzed it.

The danger of AI assistance is that it lets us skip the boring parts. But the boring parts are often where the tacit knowledge develops. Research on GPS navigation (Bohbot et al., 2020) found that heavy users showed reduced hippocampal volume and impaired spatial memory compared to those who navigated by learning routes. The tool didn't just change behavior; it changed the brain's architecture.

## Where the Argument Gets Lost

Husom's critique is strongest when grounded in these practical domains. It weakens when he veers into philosophical territory. His extended attack on the "extended mind" thesis—that cognition isn't limited to the skull but extends into the physical environment—feels like a detour that undermines his practical argument.

Claiming that "the fact that something happens in your brain rather than on a computer makes all the difference in the world" is true for the domains he's identified. But his reductio ad absurdum comparing losing your phone to having "a part of your brain cut out" risks caricaturing a more nuanced philosophical position. The extended mind thesis doesn't claim phones and brains are identical; it claims the boundary between internal and external cognition is more porous than folk psychology assumes.

The vacation planning section similarly risks alienating readers who genuinely find logistics stressful. Not everyone experiences party planning as meaningful; for some, it's pure cognitive load they'd rather spend on creative work or family time. Husom's argument is strongest when he acknowledges this—"Some may truly be in need of that crutch"—but the overall tone can read as dismissive of legitimate differences in disposition and circumstance.

The core insight—that there's a real difference between outsourcing effort and outsourcing thinking—deserves a more careful treatment. The question isn't whether tools extend cognition (they do), but whether specific tools extend it in ways that preserve or erode the capacities we value.

## The Harder Question: What About Those Who Struggle?

Husom's critique raises a question he doesn't fully address: what about people for whom writing, planning, or organizing is genuinely difficult?

He briefly acknowledges that "LLMs can certainly help people improve the text, but the thinking process—developing the ideas—will be severely amputated." But this glides past the reality that some people face genuine barriers to expression—language learners, those with learning disabilities, neurodivergent thinkers who struggle with conventional structures.

The risk of Husom's framework is that it can sound like a defense of writing as a gatekeeping mechanism: only those who struggle through the process deserve to communicate. This isn't his intent, but the argument needs clearer articulation of how to distinguish legitimate assistance from harmful dependency.

One approach: focus on locus of control. When AI helps you express ideas you already have—providing vocabulary, suggesting structure, catching errors—the thinking remains yours. When AI generates ideas you then adopt, or transforms your expression so thoroughly you no longer recognize it, the thinking has been outsourced.

The line is thin. But the distinction matters. Current chatbot interfaces make it dangerously easy to slide from one to the other without noticing.

## Toward Values-Based Technology Use

Husom's ultimate point transcends specific tools or use cases. "How we choose to use chatbots is not only about efficiency and cognitive consequences; it's about how we want our lives and society to be."

This is the question his article poses most powerfully. Technology doesn't just change what we can do; it reveals what we value by what we automate versus what we protect. If we automate writing personal messages, we're declaring that the relational aspect of communication matters less than the informational aspect. If we automate vacation planning, we're declaring that the anticipation and imagination are costs to be minimized, not experiences to be savored.

These aren't universal wrongs. They're choices. And Husom's challenge is to make those choices consciously, with awareness of what we're trading away.

The research he implicitly draws on—externalized cognition, cognitive offloading, skill atrophy—suggests these trade-offs have real consequences. They aren't just aesthetic preferences. The hippocampus doesn't atrophy because someone prefers GPS; it atrophies because navigation-by-memory is a use-it-or-lose-it capacity. Similarly, writing voice doesn't atrophy because someone values efficiency; it atrophies because finding your voice requires the struggle of expression, and removing the struggle removes the development.

## The Teaser

The solution isn't better AI tools. It's clearer boundaries.

---

## Related Reading

- [MIT Cognitive Debt Study](/research/mit-cognitive-debt-2025/) — EEG evidence of neural changes with AI writing assistance
- [Active Prompting Protocol](/cognitive-tools/ai-assisted-learning/) — Tactical framework for coding with AI while preserving skill acquisition
- [Cognitive Offloading Research](/essays/cognitive-offloading-research/) — 40-year pattern from calculators to GPS to ChatGPT
- [Philosophy](/about/philosophy/) — The Three Modes framework for human-AI collaboration

---

**Source:** [Erik Johannes Husom, "Outsourcing Thinking"](https://erikjohannes.no/posts/20260130-outsourcing-thinking/index.html) (January 30, 2026)

**Further Context:**
- Sparrow, B., Liu, J., & Wegner, D. M. (2011). Google effects on memory: Cognitive consequences of having information at our fingertips. *Science*, 333(6043), 776-778.
- Bohbot, V. D., Lerch, J., Thorndycraft, B., Iaria, G., & Zijdenbos, A. P. (2020). Gray matter differences correlate with spontaneous strategies in a human virtual navigation task. *Journal of Neuroscience*, 27(38), 10078-10083.
- Clark, A., & Chalmers, D. (1998). The extended mind. *Analysis*, 58(1), 7-19.
